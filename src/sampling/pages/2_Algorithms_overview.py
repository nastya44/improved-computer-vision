import streamlit as st


st.set_page_config(
    page_title="Gradient sampling deep learning: Algorithms overview",
    page_icon="ðŸ“–",
    layout="centered",
    initial_sidebar_state="expanded",
)


st.title("Algorithms overview")
st.header("Convolutional Neural Network")
st.image("./assets/mnist_conv.gif", caption="How convolution works on the MNIST dataset", use_column_width=True)
st.markdown(
    """A Convolutional Neural Network (CNN) is a deep learning model often used for analyzing visual imagery.
It operates by moving small filters across the input image.
These filters perform element-wise multiplication with the part of the image they cover and sum up these products.
This process creates feature maps that highlight important features in the image.
The mathematics behind this involves convolution operations, where a filter is applied to the input data.
During training, the network learns the best filter values using a process called backpropagation, which involves
calculating the gradient of the error with respect to the filter weights and adjusting them to reduce the error.
This makes CNNs effective for tasks like image recognition and classification.

Mathematically, this operation can be represented as $ (f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau) d\\tau $ 
for continuous domains, or $ (f * g)[n] = \\sum_{m=-M}^{M} f[m]g[n-m] $ for discrete domains, where $ f $ and $ g $ 
are functions (or arrays in discrete cases) representing the input image and the filter, respectively.
"""
)


st.header("Stochastic gradient descent")
st.image("./assets/sgd.gif", caption="Stochastic gradient descent convergence", use_column_width=True)
st.markdown(
    """
> **Definition 1.** The random vector $ u^k $ is designated as the stochastic gradient of the function $ f(x) $ at the point $ x = x^k $ if
the condition $ \\mathbb{E} \\{ u^k | x^k \\} = \\nabla f(x^k) $ holds true, with $ \\mathbb{E} \\{ u^k | x^k \\} $ denoting the conditional mathematical expectation.

Therefore, if $ \\nabla f(x) = \\mathbb{E} [ \\nabla_x F(x, \\xi) ] $, the vector $ u^k = \\nabla_x F(x^k, \\xi^k) $ is indeed the stochastic 
gradient of the function $ f(x) $ at the point $ x = x^k $.

For the definition of $ u^k = \\nabla_x F(x^k, \\xi^k) $, we denote it as the gradient along the $ x $ variable for the function $ F(\\cdot, \\xi^k) $, holding 
the parameter value $ \\xi = \\xi^k $ as constant. Here, $ \\rho_k $ signifies the non-negative step multipliers for the gradient, and $ \\{ \\xi^k \\} $ designates 
the independent observations (or statistics) of a random variable $ \\xi $. The corresponding stochastic gradient descent method is then expressed by the subsequent 
recurrent equation:

$$ x^{k+1} = \\Pi_X (x^k - \\rho_k u^k), \\> x^0 \\in X, \\> k \\in \\mathbb{N} $$

In order to ensure the convergence of the method given by equation, the step multipliers $ \\rho_k \\geq 0 $ are required to satisfy the conditions delineated as follows:

$$ \\sum_{k=0}^{\\infty} \\rho_k = + \\infty, \\> \\> \\> \\sum_{k=0}^{\\infty} \\rho_k^2 < + \\infty $$
"""
)

st.header("Momentum")
st.markdown(    
    """
Returning back to the analogy of gradient descent algorithm as a ball, rolling down the hill, important to mention that it's not rolling with a constant speed $ \\lambda $, but instead gaining speed with time by keeping the momentum of itself. The same approach was discovered in optimization theory: the convergence rate of an algorithm can be increased by applying a fraction $ \gamma $ of gradient from the previous iteration:

$ v_{t} = \\gamma v_{t-1} + \\lambda \\nabla_{\\theta} J(\\theta_{1}^{(i)}, ..., \\theta_{n}^{(i)}, x_{1}^{(i)}, ..., x_{n}^{(i)}, y^{(i)}) $

$ \\theta^{(i+1)} = \\theta^{(i)} - v_{t} $

The momentum term $ \\gamma $ is usually set to 0.9 or a similar value.

As mentioned in previous sections, SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around the local minimum. Momentum helps accelerate SGD in the relevant direction and dampens oscillations.
"""
)
st.image("./assets/sgd_vs_momentum.png", caption="SGD and Momentum trajectory", use_column_width=True)

st.header("Nesterov accelerated gradient (NAG)")
st.markdown(    
    """
In this approach, the momentum is not only applied to the calculated gradient value on the iteration, but also to the current parameters, which gives an approximation of their next position, like a preliminary step of the algorithm:

$ v_{i} = \\gamma v_{i-1} + \\lambda \\nabla_{\\theta} J(\\theta^{(i)} - \\gamma v_{i-1}) $

$ \\theta^{(i+1)} = \\theta^{(i)} - v_{i} $

Taking the analogy from above, if a ball is made of light material, it may slope up to the other side of a hill using its momentum. But if it is made of heavy material (like steel), it will slow down near the bottom of the hill. The NAG algorithm work by the same principle. While momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previously accumulated gradient, measures the gradient, and then makes a correction. This anticipatory update prevents us from going too fast and results in increased responsiveness.
"""
)

st.header("Gradient sampling algorithm")
st.markdown(    
    """
One of the newest approaches in general NSO is to use gradient sampling algorithms developed by Burke. The gradient sampling method (GS) is a method for minimizing an objective function that is locally Lipschitz continuous and smooth on an open dense subset $D \\in \\mathbb{R}^n$. The objective may be nonsmooth and/or nonconvex. The GS may be considered as a stabilized steepest descent algorithm. The central idea behind these techniques is to approximate the subdifferential of the objective function through random sampling of gradients near the current iteration point. The ongoing progress in the development of gradient sampling algorithms  suggests that they may have potential to rival bundle methods in the terms of theoretical might and practical performance. 


Let $f$ be a locally Lipschitz continuous function on $\\mathbb{R}^n$, and suppose that $f$ is smooth on an open dense subset $D \\in \\mathbb{R}^n$. In addition, assume that there exists a point such that the level set $lev_{Æ’(\\bar{x})} = \\{x | f(x)â‰¤ f(\\bar{x})\\}$ is compact. At a given iterate $x_k$ the gradient of the objective function is computed on a set of randomly generated nearby points $u_{kj}$ with $j \\in \\{1, 2,\\ldots, m\\}$ and $m > n + 1$. This information is utilized to construct a search direction as a vector in the convex hull of these gradients with the shortest norm. A standard line search is then used to obtain a point with lower objective function value. The stabilization of the method is controlled by the sampling radius Îµ used to sample the gradients.
The algorithm of the GS is the following:


1. independently sample $\\left\\{x^{k, 1}, \\ldots, x^{k, m}\\right\\}$ uniformly from $\\bar{B}\\left(x^k ; \\epsilon_k\\right)$ 


2. compute $\\boldsymbol{g}^k$ as the solution of $\\min _{g \\in \\mathcal{G}^k} \\frac{1}{2}\\|\\boldsymbol{g}\\|^2$, where
$$
\\mathcal{G}^k:=\\left\\{\\nabla f\\left(\\boldsymbol{x}^k\\right), \\nabla f\\left(\\boldsymbol{x}^{k, 1}\\right), \\ldots, \\nabla f\\left(\\boldsymbol{x}^{k, m}\\right)\\right\\}
$$

3. use $\\boldsymbol{g}^k$ as gradient for calculating updated parameter


"""
)
